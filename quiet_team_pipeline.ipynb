{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "48f82cc6",
      "metadata": {
        "id": "48f82cc6"
      },
      "source": [
        "# 🧠 Quiet Team Pipeline\n",
        "A complete AI pipeline to transcribe, diarize, analyze and graph multi-speaker audio."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ed8857",
      "metadata": {
        "id": "97ed8857"
      },
      "source": [
        "## 🎧 Step 1: Convert MP3 to WAV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7be1e916",
      "metadata": {
        "id": "7be1e916"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio = AudioSegment.from_mp3(MP3_FILE)\n",
        "audio = audio.set_channels(1).set_frame_rate(16000)\n",
        "audio.export(WAV_FILE, format=\"wav\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5e4a68",
      "metadata": {
        "id": "2e5e4a68"
      },
      "source": [
        "## 🗣️ Step 2: Speaker diarization with pyannote.audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a124ad05",
      "metadata": {
        "id": "a124ad05"
      },
      "outputs": [],
      "source": [
        "from pyannote.audio import Pipeline\n",
        "from collections import defaultdict\n",
        "\n",
        "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\", use_auth_token=HUGGINGFACE_TOKEN)\n",
        "diarization = pipeline(WAV_FILE)\n",
        "\n",
        "full_audio = AudioSegment.from_wav(WAV_FILE)\n",
        "speaker_segments = defaultdict(list)\n",
        "\n",
        "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
        "    speaker_segments[speaker].append((turn.start, turn.end))\n",
        "\n",
        "import os\n",
        "os.makedirs(\"diarized_speakers\", exist_ok=True)\n",
        "\n",
        "for speaker, segments in speaker_segments.items():\n",
        "    combined = AudioSegment.empty()\n",
        "    for start, end in segments[:3]:\n",
        "        combined += full_audio[start * 1000: end * 1000]\n",
        "    out_path = f\"diarized_speakers/{speaker}.wav\"\n",
        "    combined.export(out_path, format=\"wav\")\n",
        "    print(f\"Exported {out_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e574dbe",
      "metadata": {
        "id": "5e574dbe"
      },
      "source": [
        "## 🧍 Step 3: Match diarized voices with known samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb19a823",
      "metadata": {
        "id": "cb19a823"
      },
      "outputs": [],
      "source": [
        "from resemblyzer import VoiceEncoder, preprocess_wav\n",
        "import numpy as np\n",
        "\n",
        "encoder = VoiceEncoder()\n",
        "reference_embeddings = {}\n",
        "\n",
        "for file in [\"Dylan.wav\", \"Sarah.wav\"]:\n",
        "    name = file.replace(\".wav\", \"\")\n",
        "    wav = preprocess_wav(file)\n",
        "    embedding = encoder.embed_utterance(wav)\n",
        "    reference_embeddings[name] = embedding\n",
        "\n",
        "diarized_wav = preprocess_wav(\"diarized_speakers/SPEAKER_00.wav\")\n",
        "diarized_embedding = encoder.embed_utterance(diarized_wav)\n",
        "\n",
        "best_match = None\n",
        "best_score = -1\n",
        "\n",
        "for name, ref_emb in reference_embeddings.items():\n",
        "    score = np.dot(diarized_embedding, ref_emb) / (np.linalg.norm(diarized_embedding) * np.linalg.norm(ref_emb))\n",
        "    print(f\"Similarity with {name}: {score:.3f}\")\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_match = name\n",
        "\n",
        "print(f\"🧍 Most likely match: {best_match} (score: {best_score:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e139fced",
      "metadata": {
        "id": "e139fced"
      },
      "source": [
        "## ✍️ Step 4: Transcribe with Whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a21fe0",
      "metadata": {
        "id": "38a21fe0"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "transcription_data = []\n",
        "speaker_map = defaultdict(lambda: f\"Speaker {len(speaker_map) + 1}\")\n",
        "\n",
        "for i, (turn, _, speaker_label) in enumerate(diarization.itertracks(yield_label=True)):\n",
        "    segment = full_audio[turn.start * 1000: turn.end * 1000]\n",
        "    segment_path = f\"temp_segment_{i}.wav\"\n",
        "    segment.export(segment_path, format=\"wav\")\n",
        "    result = model.transcribe(segment_path, language=\"en\")\n",
        "    text = result[\"text\"].strip()\n",
        "    if not text:\n",
        "        continue\n",
        "    speaker_name = speaker_map[speaker_label]\n",
        "    transcription_data.append({\n",
        "        \"segment\": i,\n",
        "        \"start\": round(turn.start, 2),\n",
        "        \"end\": round(turn.end, 2),\n",
        "        \"speaker\": speaker_name,\n",
        "        \"text\": text\n",
        "    })\n",
        "\n",
        "with open(\"transcription_with_speakers.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(transcription_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"✅ Transcription saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Step 5: Build a semantic graph from transcription\n"
      ],
      "metadata": {
        "id": "z8QmaK6YBvRI"
      },
      "id": "z8QmaK6YBvRI"
    },
    {
      "cell_type": "code",
      "source": [
        "# === STEP 5: Build a semantic graph from transcription ===\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "G = nx.Graph()\n",
        "nodes = []\n",
        "embeddings = []\n",
        "\n",
        "for i, segment in enumerate(speaker_data):\n",
        "    node_id = f\"{segment['speaker']}_msg{i}\"\n",
        "    text = segment['text']\n",
        "    G.add_node(node_id, speaker=segment['speaker'], text=text)\n",
        "    nodes.append((node_id, text))\n",
        "    embeddings.append(model.encode(text))\n",
        "\n",
        "sim_matrix = cosine_similarity(embeddings)\n",
        "threshold = 0.6\n",
        "\n",
        "for i in range(len(nodes)):\n",
        "    for j in range(i + 1, len(nodes)):\n",
        "        sim = sim_matrix[i][j]\n",
        "        if sim > threshold:\n",
        "            G.add_edge(nodes[i][0], nodes[j][0], weight=sim)"
      ],
      "metadata": {
        "id": "PGd1w1vOBkJm"
      },
      "id": "PGd1w1vOBkJm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔗 Step 6: Push graph to Neo4j"
      ],
      "metadata": {
        "id": "FLnRRwaPCI6M"
      },
      "id": "FLnRRwaPCI6M"
    },
    {
      "cell_type": "code",
      "source": [
        "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
        "\n",
        "def create_graph_in_neo4j(tx, G):\n",
        "    tx.run(\"MATCH (n) DETACH DELETE n\")\n",
        "    for node_id, data in G.nodes(data=True):\n",
        "        tx.run(\"\"\"\n",
        "            CREATE (:Phrase {id: $id, text: $text, speaker: $speaker})\n",
        "        \"\"\", id=node_id, text=data[\"text\"], speaker=data[\"speaker\"])\n",
        "\n",
        "    for source, target, data in G.edges(data=True):\n",
        "        tx.run(\"\"\"\n",
        "            MATCH (a:Phrase {id: $id1})\n",
        "            MATCH (b:Phrase {id: $id2})\n",
        "            CREATE (a)-[:SIMILAR_TO {weight: $weight}]->(b)\n",
        "        \"\"\", id1=source, id2=target, weight=data[\"weight\"])\n",
        "\n",
        "with driver.session() as session:\n",
        "    session.write_transaction(create_graph_in_neo4j, G)\n",
        "    print(\"✅ Graph successfully written to Neo4j!\")"
      ],
      "metadata": {
        "id": "HhOvtOO-CIU3"
      },
      "id": "HhOvtOO-CIU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 💬 Step 7: Query the knowledge graph with LangChain"
      ],
      "metadata": {
        "id": "J2qXdWp1CIEE"
      },
      "id": "J2qXdWp1CIEE"
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [\n",
        "    Document(page_content=data[\"text\"], metadata={\"speaker\": data[\"speaker\"]})\n",
        "    for _, data in G.nodes(data=True)\n",
        "]\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)\n",
        "\n",
        "query = \"How could national economic interests have been protected without triggering global inflation and collateral damage?\"\n",
        "result = qa_chain(query)\n",
        "print(\"📣 Answer:\", result[\"result\"])"
      ],
      "metadata": {
        "id": "XFD_8I2ICWiv"
      },
      "id": "XFD_8I2ICWiv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}